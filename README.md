# Feature-engineering-one-hot-encoding
In my analysis, I employed one-hot encoding for nominal data that doesn't have any inherent rank. To avoid the dummy variable trap, I dropped the first column of the one-hot encoded data. Additionally, I reduced dimensionality by focusing on the most frequently occurring features and consolidating the less common ones into a separate category. This approach helped streamline the dataset and improve the efficiency of the machine learning models.





